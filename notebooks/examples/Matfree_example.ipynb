{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaAJcnWe8XBi",
        "outputId": "aacc27b1-50e8-4cd5-d812-3792ef36c710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tree-math\n",
            "  Downloading tree_math-0.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from tree-math) (0.4.20)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->tree-math) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax->tree-math) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->tree-math) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->tree-math) (1.11.3)\n",
            "Installing collected packages: tree-math\n",
            "Successfully installed tree-math-0.2.0\n",
            "Collecting matfree\n",
            "  Downloading matfree-0.0.9-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: matfree\n",
            "Successfully installed matfree-0.0.9\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import argparse\n",
        "\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import optax\n",
        "!pip install tree-math\n",
        "!pip install matfree\n",
        "import matfree\n",
        "import tree_math as tm\n",
        "from flax import linen as nn\n",
        "from jax import nn as jnn\n",
        "from jax import numpy as jnp\n",
        "from jax import random, jit\n",
        "import pickle\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Stuff"
      ],
      "metadata": {
        "id": "K8gz8MnN_tOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep MNIST"
      ],
      "metadata": {
        "id": "LgX_xGtB_tYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision.transforms.functional import normalize\n",
        "\n",
        "\n",
        "def select_num_samples(dataset, n_samples, cls_to_idx):\n",
        "    idxs = []\n",
        "    for key,_ in cls_to_idx.items():\n",
        "        indices = np.where(dataset.targets == key)[0]\n",
        "        idxs.append(np.random.choice(indices, n_samples, replace=False))\n",
        "    idxs = np.concatenate(idxs)\n",
        "    dataset.data = dataset.data[idxs]\n",
        "    dataset.targets = dataset.targets[idxs]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def select_classes(dataset, classes):\n",
        "    idxs = []\n",
        "    for i in classes:\n",
        "        indices = np.where(dataset.targets == i)[0]\n",
        "        idxs.append(indices)\n",
        "    idxs = np.concatenate(idxs).astype(int)\n",
        "    dataset.data = dataset.data[idxs]\n",
        "    dataset.targets = dataset.targets[idxs]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def numpy_collate_fn(batch):\n",
        "    data, target = zip(*batch)\n",
        "    data = np.stack(data)\n",
        "    target = np.stack(target)\n",
        "    return {\"image\": data, \"label\": target}\n",
        "\n",
        "\n",
        "def channel_normalization(tensor, mean, std):\n",
        "    tensor = torch.from_numpy(tensor).float().transpose(1, 3)\n",
        "    tensor = normalize(tensor, mean, std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class MNIST(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_root=\"/data/\",\n",
        "        train: bool = True,\n",
        "        transform = None,\n",
        "        n_samples: int = None,\n",
        "        cls: list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "        download=True,\n",
        "    ):\n",
        "        self.path = Path(path_root)\n",
        "        if train:\n",
        "            self.dataset = tv.datasets.MNIST(root=self.path, train=True, download=download)\n",
        "        else:\n",
        "            self.dataset = tv.datasets.MNIST(root=self.path, train=False, download=download)\n",
        "        self.transfrm = transform\n",
        "\n",
        "        clas_to_index = { c : i for i, c in enumerate(cls)}\n",
        "        if len(cls)<10:\n",
        "                self.dataset = select_classes(self.dataset, cls)\n",
        "        if n_samples is not None:\n",
        "            self.dataset = select_num_samples(self.dataset, n_samples, clas_to_index)\n",
        "\n",
        "        self.dataset.targets = torch.tensor([clas_to_index[clas.item()] for clas in self.dataset.targets])\n",
        "\n",
        "        self.data, self.targets = (self.dataset.data.float().unsqueeze(-1) / 255.0).transpose(1, 3).numpy(), F.one_hot(\n",
        "            self.dataset.targets, len(cls)\n",
        "        ).numpy()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        if self.transfrm is not None:\n",
        "            img = self.transfrm(torch.from_numpy(img)).numpy()\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "iR2J-2zv9hyU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = 1000\n",
        "classes_train = [0,1,2,3,4,5,6,7,8,9]\n",
        "n_classes = 10\n",
        "batch_size = 256\n",
        "test_batch_size = 256\n",
        "\n",
        "data_train = MNIST(path_root= \"data/\",\n",
        "            train=True, n_samples=train_samples if train_samples > 0 else None, cls=classes_train\n",
        "        )\n",
        "data_test = MNIST(path_root = \"/data/\", train=False, cls=classes_train)\n",
        "\n",
        "if train_samples > 0:\n",
        "    N = train_samples * n_classes\n",
        "else:\n",
        "    N = len(data_train)\n",
        "N_test = len(data_test)\n",
        "if test_batch_size > 0:\n",
        "    test_batch_size = test_batch_size\n",
        "else:\n",
        "    test_batch_size = len(data_test)\n",
        "\n",
        "n_test_batches = int(N_test / test_batch_size)\n",
        "n_batches = int(N / batch_size)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    data_train, batch_size=batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    data_test, batch_size=test_batch_size, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
        ")"
      ],
      "metadata": {
        "id": "nxopeVK7AQ6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9b7d44-3165-4a40-8f74-1ed2e4e7b941"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 179948716.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27764312.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 44023064.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15875440.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 115208386.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/train-images-idx3-ubyte.gz to /data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28495811.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/train-labels-idx1-ubyte.gz to /data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 19241756.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/t10k-images-idx3-ubyte.gz to /data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13834806.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Convnet\n"
      ],
      "metadata": {
        "id": "IT7MVgHXBaay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    output_dim: int = 10\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "            x = jnp.expand_dims(x, 0)\n",
        "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
        "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = nn.Conv(features=4, kernel_size=(3, 3), strides=(2, 2), padding=1)(x)\n",
        "        x = nn.tanh(x)\n",
        "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "        x = x.reshape((x.shape[0], -1))\n",
        "        return nn.Dense(features=self.output_dim)(x)\n",
        "\n",
        "def compute_num_params(pytree):\n",
        "    return sum(x.size if hasattr(x, \"size\") else 0 for x in jax.tree_util.tree_leaves(pytree))\n",
        "\n",
        "\n",
        "model = ConvNet()\n",
        "batch = next(iter(train_loader))\n",
        "x_init, y_init = batch[\"image\"], batch[\"label\"]\n",
        "output_dim = y_init.shape[-1]\n",
        "key, split_key = random.split(jax.random.PRNGKey(0))\n",
        "params = model.init(key, x_init)\n",
        "alpha = 1.\n",
        "optim = optax.chain(\n",
        "        optax.clip(1.),\n",
        "        getattr(optax, \"adam\")(1e-2),\n",
        "    )\n",
        "opt_state = optim.init(params)\n",
        "n_params = compute_num_params(params)\n",
        "n_epochs = 100"
      ],
      "metadata": {
        "id": "tfViL5h3BfEK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(preds, y, rho=1.0):\n",
        "    \"\"\"\n",
        "    preds: (n_samples, n_classes) (logits)\n",
        "    y: (n_samples, n_classes) (one-hot labels)\n",
        "    \"\"\"\n",
        "    preds = preds * rho\n",
        "    preds = jax.nn.log_softmax(preds, axis=-1)\n",
        "    return -jnp.sum(jnp.sum(preds * y, axis=-1))\n",
        "\n",
        "def accuracy(params, model, batch_x, batch_y):\n",
        "    preds = model.apply(params, batch_x)\n",
        "    return jnp.sum(preds.argmax(axis=-1) == batch_y.argmax(axis=-1))\n",
        "\n",
        "\n",
        "def map_loss(\n",
        "    params,\n",
        "    model,\n",
        "    x_batch,\n",
        "    y_batch,\n",
        "    alpha,\n",
        "    n_params: int,\n",
        "    N_datapoints_max: int,\n",
        "):\n",
        "    # define dict for logging purposes\n",
        "    B = x_batch.shape[0]\n",
        "    O = y_batch.shape[-1]\n",
        "    D = n_params\n",
        "    N = N_datapoints_max\n",
        "\n",
        "    # hessian_scaler = 1\n",
        "\n",
        "    vparams = tm.Vector(params)\n",
        "\n",
        "    rho = 1.\n",
        "    nll = lambda x, y, rho: 1/B * cross_entropy_loss(x, y, rho)\n",
        "\n",
        "    y_pred = model.apply(params, x_batch)\n",
        "\n",
        "    loglike_loss = nll(y_pred, y_batch, rho) #* hessian_scaler\n",
        "\n",
        "    log_prior_term = -D / 2 * jnp.log(2 * jnp.pi) - (1 / 2) * alpha * (vparams @ vparams) + D / 2 * jnp.log(alpha)\n",
        "    # log_det_term = 0\n",
        "    loss = loglike_loss - 0. * log_prior_term\n",
        "\n",
        "    return loss\n",
        "\n",
        "def make_step(params, alpha, opt_state, x, y):\n",
        "    grad_fn = jax.value_and_grad(map_loss, argnums=0, has_aux=False)\n",
        "    loss, grads = grad_fn(params, model, x, y, alpha, n_params, N)\n",
        "    param_updates, opt_state = optim.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, param_updates)\n",
        "    return loss, params, opt_state\n",
        "\n",
        "jit_make_step = jit(make_step)\n",
        "\n"
      ],
      "metadata": {
        "id": "DpGW3sE0BnJL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, n_epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    start_time = time.time()\n",
        "    for _, batch in zip(range(n_batches), train_loader):\n",
        "        X = batch[\"image\"]\n",
        "        y = batch[\"label\"]\n",
        "        B = X.shape[0]\n",
        "        train_key, split_key = random.split(split_key)\n",
        "\n",
        "        loss, params, opt_state = jit_make_step(params, alpha, opt_state, X, y)\n",
        "        loss = loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        epoch_accuracy += accuracy(params, model, X, y).item()\n",
        "\n",
        "    epoch_accuracy /= (n_batches * B)\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(\n",
        "        f\"epoch={epoch}, loss={epoch_loss:.3f}, , accuracy={epoch_accuracy:.2f}, alpha={alpha:.2f}, time={epoch_time:.3f}s\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSLNO7PhBqD8",
        "outputId": "aceb9a31-c2fe-471f-82cb-007f029f11ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=1, loss=81.008, , accuracy=0.30, alpha=1.00, time=6.988s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling!!"
      ],
      "metadata": {
        "id": "CTVV83OWB_b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_train_loader = torch.utils.data.DataLoader(\n",
        "    data_train, batch_size=N, shuffle=True, collate_fn=numpy_collate_fn, drop_last=True,\n",
        ")\n",
        "data = next(iter(sampling_train_loader))\n",
        "x_train = jnp.array(data[\"image\"])\n",
        "y_train = jnp.array(data[\"label\"])\n",
        "sample_key = jax.random.PRNGKey(0)\n",
        "n_posterior_samples = 200\n",
        "num_iterations = 1\n",
        "n_sample_batch_size = 1\n",
        "n_sample_batches = N // n_sample_batch_size"
      ],
      "metadata": {
        "id": "HUMTxiaIDAkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ground Truth"
      ],
      "metadata": {
        "id": "aGCDcJbWCsA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "def calculate_exact_ggn(loss_fn, model_fn, params, X, y, n_params):\n",
        "    def body_fun(carry, a_tuple):\n",
        "        x, y = a_tuple\n",
        "        my_model_fn = partial(model_fn, x=x)  # model_fn wrt parameters\n",
        "        my_loss_fn = partial(loss_fn, y=y)  # loss_fn wrt model output\n",
        "        pred = my_model_fn(params)\n",
        "        jacobian = jax.jacfwd(my_model_fn)(params)\n",
        "        jacobian = jax.tree_map(lambda x: jnp.reshape(x, (x.shape[0], -1)), jacobian)\n",
        "        jacobian = jnp.concatenate(jax.tree_util.tree_flatten(jacobian)[0], axis=-1)\n",
        "        loss_hessian = jax.hessian(my_loss_fn)(pred)\n",
        "        ggn = jacobian.T @ loss_hessian @ jacobian\n",
        "        return jax.tree_map(lambda a, b: a + b, carry, ggn), None\n",
        "\n",
        "    init_value = jnp.zeros((n_params, n_params))  # jacobian.T @ loss_hessian @ jacobian\n",
        "    return jax.lax.scan(body_fun, init_value, (X, y))[0]\n"
      ],
      "metadata": {
        "id": "If1VnxToCuX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_model_fn = lambda params, x: model.apply(params, x[None, ...])[0]\n",
        "ggn = calculate_exact_ggn(cross_entropy_loss, _model_fn, params, x_train, y_train, n_params)\n"
      ],
      "metadata": {
        "id": "fVeSvGOxC-FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low Rank Approximation"
      ],
      "metadata": {
        "id": "58Ddvr6ACPoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matfree\n",
        "from matfree import decomp, lanczos, montecarlo\n",
        "from matfree.backend import func, linalg, np\n",
        "from typing import Callable, Literal, Optional\n",
        "\n",
        "def lanczos_tridiag(\n",
        "        Av: Callable,\n",
        "        v0: jax.Array,\n",
        "        order: int\n",
        "):\n",
        "    ncols = v0.shape[0]\n",
        "    if order >= ncols or order < 1:\n",
        "        raise ValueError\n",
        "    algorithm = matfree.lanczos.tridiagonal_full_reortho(order)\n",
        "    u0 = v0/jnp.linalg.norm(v0)\n",
        "    _, tridiag = decomp.decompose_fori_loop(u0, Av, algorithm=algorithm)\n",
        "    (diag, off_diag) = tridiag\n",
        "    diag = linalg.diagonal_matrix(diag)\n",
        "    offdiag1 = linalg.diagonal_matrix(off_diag, -1)\n",
        "    offdiag2 = linalg.diagonal_matrix(off_diag, 1)\n",
        "    dense_matrix = diag + offdiag1 + offdiag2\n",
        "    eigvals, _ = linalg.eigh(dense_matrix)\n",
        "    return eigvals"
      ],
      "metadata": {
        "id": "jrZPuJ-UCgu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests"
      ],
      "metadata": {
        "id": "EOWuvAU1D4Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GGN"
      ],
      "metadata": {
        "id": "OH2YXjZJqgWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v0 = jnp.ones(n_params)\n",
        "order = 100\n",
        "eigvals = lanczos_tridiag(lambda v: ggn @ v, v0, order)\n"
      ],
      "metadata": {
        "id": "NkgD0dL1EHJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigvals_gt, _ = jnp.linalg.eigh(ggn)\n"
      ],
      "metadata": {
        "id": "COs6yJtsqefF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(eigvals, marker=\"o\", linestyle=None, color=\"k\", label=\"Lanczos\")\n",
        "plt.plot(eigvals_gt[-order:], marker=\"x\", linestyle=None, label=\"Ground Truth\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OVb8V5IYrPoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "422EOIXUwFRs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}